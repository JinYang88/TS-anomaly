{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pleasant-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fiscal-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from common.evaluation import iter_thresholds\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "equal-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_dict = {\n",
    "    \"Group A\": {\"machine-1-1\": 38},\n",
    "    \"Group B\": {\"machine-1-1\": 38},\n",
    "    \"Group C\": {\"machine-1-1\": 38}\n",
    "}\n",
    "# summarize_dict = {\"Group A\": {\"e29ca1cd\": 3,\n",
    "# \"c23b2b2d\": 4,\n",
    "# \"aeb5a1de\": 6,\n",
    "# \"2fe95315\": 5,\n",
    "# \"0a82a873\": 7,\n",
    "# \"af732cc4\": 7},\n",
    "# \"Group B\": {\"b2a04b7f\": 20,\n",
    "# \"c2970798\": 13,\n",
    "# \"5dafb960\": 12},\n",
    "# \"Group C\": {\"c91f4a07\": 24,\n",
    "# \"ca2ae31d\": 43,\n",
    "# \"f7958fb7\": 37}}\n",
    "results_dir = \"../benchmark/benchmarking_results/\"\n",
    "models = [\"3sigma\", \"iforest\",  \"LODA\", \"PCA\", \"AutoEncoder\", \"lstm\", \"lstm_vae\", \"dagmm\", \"omnianomaly\", \"CMAnomaly_old\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "narrow-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tuple(path, nrows=None):\n",
    "    anomaly_score = np.load(\n",
    "        os.path.join(path, \"anomaly_score.npz\"), allow_pickle=True\n",
    "    )[\"arr_0\"].item()[\"test\"]\n",
    "    anomaly_label = np.load(os.path.join(path, \"anomaly_label.npz\"))[\n",
    "        \"arr_0\"\n",
    "    ].astype(int)\n",
    "    return (anomaly_score[:nrows], anomaly_label[:nrows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "clean-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(anomaly_score, anomaly_label):\n",
    "    _, _, best_adjust_pred, _ = iter_thresholds(anomaly_score, anomaly_label, metric=\"f1\", adjustment=True)\n",
    "    _, _, best_raw_pred, _ = iter_thresholds(anomaly_score, anomaly_label, metric=\"f1\", adjustment=False)\n",
    "    \n",
    "    aF1 = f1_score(anomaly_label, best_adjust_pred)\n",
    "    aPC = precision_score(anomaly_label, best_adjust_pred)\n",
    "    aRC = recall_score(anomaly_label, best_adjust_pred)\n",
    "    \n",
    "    rF1 = f1_score(anomaly_label, best_raw_pred)\n",
    "    rPC = precision_score(anomaly_label, best_raw_pred)\n",
    "    rRC = recall_score(anomaly_label, best_raw_pred)\n",
    "    return [aF1, aPC, aRC, rF1, rPC, rRC] \n",
    "\n",
    "def concat_keys(value_dict, keys):\n",
    "    anomaly_score_con = np.concatenate([value_dict[k][\"anomaly_score\"] for k in keys])\n",
    "    anomaly_label_con = np.concatenate([value_dict[k][\"anomaly_label\"] for k in keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "persistent-yemen",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish collecting.\n",
      "Group A ['machine-1-1']\n",
      "Group B ['machine-1-1']\n",
      "Group C ['machine-1-1']\n",
      "Finish individuall evaluation\n"
     ]
    }
   ],
   "source": [
    "df_dict = {}\n",
    "nrows = 100\n",
    "model = \"3sigma\"\n",
    "\n",
    "df_dict[\"model\"] = model\n",
    "folders = glob(os.path.join(results_dir, f\"{model}/*/*/*\"))\n",
    "subdataset_values = {}\n",
    "metrics_save = defaultdict(list)\n",
    "for folder in folders:\n",
    "    folder_components = folder.split(os.sep)\n",
    "    dataset, subdataset = folder_components[-2], folder_components[-1]\n",
    "    df_dict[\"dataset\"] = dataset\n",
    "    anomaly_score, anomaly_label = load_tuple(folder, nrows)\n",
    "    subdataset_values[subdataset] = {\n",
    "        \"anomaly_score\": anomaly_score,\n",
    "        \"anomaly_label\": anomaly_label,\n",
    "    }\n",
    "print(\"Finish collecting.\")\n",
    "for group_name, subdatasets in summarize_dict.items():\n",
    "    subdatasets = list(subdatasets.keys())\n",
    "    print(group_name, subdatasets)\n",
    "\n",
    "print(\"Finish individuall evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "selected-vanilla",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': '3sigma',\n",
       " 'dataset': 'SMD',\n",
       " 'aF1': 0.0,\n",
       " 'aPC': 0.0,\n",
       " 'aRC': 0.0,\n",
       " 'rF1': 0.0,\n",
       " 'rPC': 0.0,\n",
       " 'rRC': 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict.update({k: sum(v)/len(v) for k,v in metrics_save.items()}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dangerous-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_score_con = np.concatenate([v[\"anomaly_score\"] for k, v in subdataset_values.items()])\n",
    "anomaly_label_con = np.concatenate([v[\"anomaly_label\"] for k, v in subdataset_values.items()])\n",
    "_, _, best_adjust_pred, _ = iter_thresholds(anomaly_score_con, anomaly_label_con, metric=\"f1\", adjustment=True)\n",
    "_, _, best_raw_pred, _ = iter_thresholds(anomaly_score_con, anomaly_label_con, metric=\"f1\", adjustment=False)\n",
    "\n",
    "df_dict[\"aF1_con\"] = f1_score(anomaly_label_con, best_adjust_pred)\n",
    "df_dict[\"aPC_con\"] = precision_score(anomaly_label_con, best_adjust_pred)\n",
    "df_dict[\"aRC_con\"] = recall_score(anomaly_label_con, best_adjust_pred)\n",
    "\n",
    "df_dict[\"rF1_con\"] = f1_score(anomaly_label_con, best_raw_pred)\n",
    "df_dict[\"rPC_con\"] = precision_score(anomaly_label_con, best_raw_pred)\n",
    "df_dict[\"rRC_con\"] = recall_score(anomaly_label_con, best_raw_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "infectious-acquisition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': '3sigma',\n",
       " 'dataset': 'SMD',\n",
       " 'aF1': 0.0,\n",
       " 'aPC': 0.0,\n",
       " 'aRC': 0.0,\n",
       " 'rF1': 0.0,\n",
       " 'rPC': 0.0,\n",
       " 'rRC': 0.0,\n",
       " 'aF1_con': 0.0,\n",
       " 'aPC_con': 0.0,\n",
       " 'aRC_con': 0.0,\n",
       " 'rF1_con': 0.0,\n",
       " 'rPC_con': 0.0,\n",
       " 'rRC_con': 0.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-short",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
