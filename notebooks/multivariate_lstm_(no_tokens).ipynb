{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "sys.argv=['']\n",
    "del sys\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython import embed\n",
    "from common import data_preprocess \n",
    "from common.utils import print_to_json, iter_thresholds\n",
    "from common.dataloader import load_dataset\n",
    "from common.sliding import WindowIterator\n",
    "from common.config import parse_arguments, set_logger, initialize_config\n",
    "from networks.mlstm import MultiLSTMEncoder\n",
    "from torch import nn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "args = parse_arguments()\n",
    "\n",
    "# load config\n",
    "config_dir = \"./hypers/\" if not args[\"load\"] else args[\"load\"]\n",
    "params = initialize_config(config_dir, args)\n",
    "params[\"clear\"] = 1\n",
    "params[\"nrows\"] = None\n",
    "params[\"inter\"] = \"MEAN\"\n",
    "params[\"window_size\"] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 00:18:47,052 P25707 INFO Loading machine-2-4 of SMD dataset\n",
      "2021-02-22 00:18:47,053 P25707 INFO 1 files found.\n"
     ]
    }
   ],
   "source": [
    "data_dict = load_dataset(\"SMD\",\"machine-2-4\", use_dim=\"all\")\n",
    "# data_dict[\"train\"].max(), data_dict[\"train\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 00:18:47,242 P25707 INFO Saving preprocessor into ./checkpoints/20210222-001846/preprocessor.pkl\n",
      "2021-02-22 00:18:47,244 P25707 INFO Normalizing data\n",
      "2021-02-22 00:18:47,261 P25707 INFO Generating sliding windows (size 32).\n",
      "2021-02-22 00:18:47,367 P25707 INFO Train windows #: (4732, 32, 38)\n",
      "2021-02-22 00:18:47,368 P25707 INFO Test windows #: (23657, 32, 38)\n"
     ]
    }
   ],
   "source": [
    "pp = data_preprocess.preprocessor()\n",
    "pp.save(params[\"save_path\"])\n",
    "data_dict = pp.normalize(data_dict,method=\"minmax\")\n",
    "window_dict = data_preprocess.generate_windows(data_dict, data_hdf5_path=params[\"path\"], **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0, 1.0, 0.0)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[\"train\"].max(), data_dict[\"train\"].min(), data_dict[\"test\"].max(), data_dict[\"test\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4732, 32, 38)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_dict[\"train_windows\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterator = WindowIterator(window_dict[\"train_windows\"], batch_size=params[\"batch_size\"], shuffle=True)\n",
    "test_iterator = WindowIterator(window_dict[\"test_windows\"], batch_size=params[\"batch_size\"], shuffle=False)\n",
    "params['in_channels'] = data_dict[\"dim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 00:18:47,963 P25707 INFO Compiling finished.\n"
     ]
    }
   ],
   "source": [
    "params[\"nb_steps\"] = 200\n",
    "encoder = MultiLSTMEncoder(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 00:18:48,133 P25707 INFO Start training for 74 batches.\n",
      "2021-02-22 00:18:48,361 P25707 INFO Epoch: 1, loss: 54.57195\n",
      "2021-02-22 00:18:48,544 P25707 INFO Epoch: 2, loss: 16.00438\n",
      "2021-02-22 00:18:48,758 P25707 INFO Epoch: 3, loss: 13.09840\n",
      "2021-02-22 00:18:48,951 P25707 INFO Epoch: 4, loss: 12.01773\n",
      "2021-02-22 00:18:49,128 P25707 INFO Epoch: 5, loss: 11.38449\n",
      "2021-02-22 00:18:49,307 P25707 INFO Epoch: 6, loss: 10.88530\n",
      "2021-02-22 00:18:49,493 P25707 INFO Epoch: 7, loss: 10.61105\n",
      "2021-02-22 00:18:49,671 P25707 INFO Epoch: 8, loss: 10.26685\n",
      "2021-02-22 00:18:49,853 P25707 INFO Epoch: 9, loss: 9.95500\n",
      "2021-02-22 00:18:50,034 P25707 INFO Epoch: 10, loss: 9.77768\n",
      "2021-02-22 00:18:50,218 P25707 INFO Epoch: 11, loss: 9.53253\n",
      "2021-02-22 00:18:50,445 P25707 INFO Epoch: 12, loss: 9.36402\n",
      "2021-02-22 00:18:50,654 P25707 INFO Epoch: 13, loss: 9.28466\n",
      "2021-02-22 00:18:50,883 P25707 INFO Epoch: 14, loss: 9.26840\n",
      "2021-02-22 00:18:51,134 P25707 INFO Epoch: 15, loss: 8.99486\n",
      "2021-02-22 00:18:51,332 P25707 INFO Epoch: 16, loss: 8.83376\n",
      "2021-02-22 00:18:51,530 P25707 INFO Epoch: 17, loss: 8.73290\n",
      "2021-02-22 00:18:51,746 P25707 INFO Epoch: 18, loss: 8.74405\n",
      "2021-02-22 00:18:51,940 P25707 INFO Epoch: 19, loss: 8.59943\n",
      "2021-02-22 00:18:52,137 P25707 INFO Epoch: 20, loss: 8.51792\n",
      "2021-02-22 00:18:52,325 P25707 INFO Epoch: 21, loss: 8.52472\n",
      "2021-02-22 00:18:52,620 P25707 INFO Epoch: 22, loss: 8.46302\n",
      "2021-02-22 00:18:52,864 P25707 INFO Epoch: 23, loss: 8.27758\n",
      "2021-02-22 00:18:53,084 P25707 INFO Epoch: 24, loss: 8.32896\n",
      "2021-02-22 00:18:53,348 P25707 INFO Epoch: 25, loss: 8.17281\n",
      "2021-02-22 00:18:53,610 P25707 INFO Epoch: 26, loss: 8.15780\n",
      "2021-02-22 00:18:53,885 P25707 INFO Epoch: 27, loss: 8.08667\n",
      "2021-02-22 00:18:54,180 P25707 INFO Epoch: 28, loss: 8.01703\n",
      "2021-02-22 00:18:54,466 P25707 INFO Epoch: 29, loss: 7.94596\n",
      "2021-02-22 00:18:54,750 P25707 INFO Epoch: 30, loss: 8.01168\n",
      "2021-02-22 00:18:55,008 P25707 INFO Epoch: 31, loss: 7.90561\n",
      "2021-02-22 00:18:55,255 P25707 INFO Epoch: 32, loss: 7.96854\n",
      "2021-02-22 00:18:55,521 P25707 INFO Epoch: 33, loss: 7.81487\n",
      "2021-02-22 00:18:55,775 P25707 INFO Epoch: 34, loss: 7.86791\n",
      "2021-02-22 00:18:56,009 P25707 INFO Epoch: 35, loss: 7.75646\n",
      "2021-02-22 00:18:56,267 P25707 INFO Epoch: 36, loss: 7.66736\n",
      "2021-02-22 00:18:56,518 P25707 INFO Epoch: 37, loss: 7.58285\n",
      "2021-02-22 00:18:56,785 P25707 INFO Epoch: 38, loss: 7.65121\n",
      "2021-02-22 00:18:57,044 P25707 INFO Epoch: 39, loss: 7.52277\n",
      "2021-02-22 00:18:57,304 P25707 INFO Epoch: 40, loss: 7.64863\n",
      "2021-02-22 00:18:57,588 P25707 INFO Epoch: 41, loss: 7.65895\n",
      "2021-02-22 00:18:57,858 P25707 INFO Epoch: 42, loss: 7.52862\n",
      "2021-02-22 00:18:58,155 P25707 INFO Epoch: 43, loss: 7.58909\n",
      "2021-02-22 00:18:58,419 P25707 INFO Epoch: 44, loss: 7.54530\n",
      "2021-02-22 00:18:58,683 P25707 INFO Epoch: 45, loss: 7.46153\n",
      "2021-02-22 00:18:58,955 P25707 INFO Epoch: 46, loss: 7.46623\n",
      "2021-02-22 00:18:59,224 P25707 INFO Epoch: 47, loss: 7.37605\n",
      "2021-02-22 00:18:59,508 P25707 INFO Epoch: 48, loss: 7.47500\n",
      "2021-02-22 00:18:59,768 P25707 INFO Epoch: 49, loss: 7.35448\n",
      "2021-02-22 00:19:00,037 P25707 INFO Epoch: 50, loss: 7.27463\n",
      "2021-02-22 00:19:00,301 P25707 INFO Epoch: 51, loss: 7.30124\n",
      "2021-02-22 00:19:00,578 P25707 INFO Epoch: 52, loss: 7.24489\n",
      "2021-02-22 00:19:00,848 P25707 INFO Epoch: 53, loss: 7.33939\n",
      "2021-02-22 00:19:01,130 P25707 INFO Epoch: 54, loss: 7.30166\n",
      "2021-02-22 00:19:01,391 P25707 INFO Epoch: 55, loss: 7.32301\n",
      "2021-02-22 00:19:01,660 P25707 INFO Epoch: 56, loss: 7.31855\n",
      "2021-02-22 00:19:01,929 P25707 INFO Epoch: 57, loss: 7.24096\n",
      "2021-02-22 00:19:02,203 P25707 INFO Epoch: 58, loss: 7.31016\n",
      "2021-02-22 00:19:02,483 P25707 INFO Epoch: 59, loss: 7.22029\n",
      "2021-02-22 00:19:02,754 P25707 INFO Epoch: 60, loss: 7.19430\n",
      "2021-02-22 00:19:03,028 P25707 INFO Epoch: 61, loss: 7.30439\n",
      "2021-02-22 00:19:03,311 P25707 INFO Epoch: 62, loss: 7.16456\n",
      "2021-02-22 00:19:03,570 P25707 INFO Epoch: 63, loss: 7.16052\n",
      "2021-02-22 00:19:03,831 P25707 INFO Epoch: 64, loss: 7.17610\n",
      "2021-02-22 00:19:04,107 P25707 INFO Epoch: 65, loss: 7.11053\n",
      "2021-02-22 00:19:04,367 P25707 INFO Epoch: 66, loss: 7.20850\n",
      "2021-02-22 00:19:04,631 P25707 INFO Epoch: 67, loss: 7.07802\n",
      "2021-02-22 00:19:04,898 P25707 INFO Epoch: 68, loss: 7.10254\n",
      "2021-02-22 00:19:05,196 P25707 INFO Epoch: 69, loss: 6.99705\n",
      "2021-02-22 00:19:05,496 P25707 INFO Epoch: 70, loss: 7.02702\n",
      "2021-02-22 00:19:05,769 P25707 INFO Epoch: 71, loss: 7.07621\n",
      "2021-02-22 00:19:06,034 P25707 INFO Epoch: 72, loss: 7.18722\n",
      "2021-02-22 00:19:06,421 P25707 INFO Epoch: 73, loss: 7.09011\n",
      "2021-02-22 00:19:06,718 P25707 INFO Epoch: 74, loss: 7.01929\n",
      "2021-02-22 00:19:06,938 P25707 INFO Epoch: 75, loss: 7.00041\n",
      "2021-02-22 00:19:07,198 P25707 INFO Epoch: 76, loss: 6.95722\n",
      "2021-02-22 00:19:07,455 P25707 INFO Epoch: 77, loss: 6.98795\n",
      "2021-02-22 00:19:07,678 P25707 INFO Epoch: 78, loss: 7.05650\n",
      "2021-02-22 00:19:07,917 P25707 INFO Epoch: 79, loss: 6.93648\n",
      "2021-02-22 00:19:08,226 P25707 INFO Epoch: 80, loss: 6.89024\n",
      "2021-02-22 00:19:08,490 P25707 INFO Epoch: 81, loss: 7.00911\n",
      "2021-02-22 00:19:08,725 P25707 INFO Epoch: 82, loss: 6.87433\n",
      "2021-02-22 00:19:09,007 P25707 INFO Epoch: 83, loss: 6.93545\n",
      "2021-02-22 00:19:09,314 P25707 INFO Epoch: 84, loss: 6.91604\n",
      "2021-02-22 00:19:09,548 P25707 INFO Epoch: 85, loss: 6.85671\n",
      "2021-02-22 00:19:09,777 P25707 INFO Epoch: 86, loss: 6.93539\n",
      "2021-02-22 00:19:10,015 P25707 INFO Epoch: 87, loss: 6.76931\n",
      "2021-02-22 00:19:10,256 P25707 INFO Epoch: 88, loss: 6.77093\n",
      "2021-02-22 00:19:10,483 P25707 INFO Epoch: 89, loss: 6.84512\n",
      "2021-02-22 00:19:10,716 P25707 INFO Epoch: 90, loss: 6.90688\n",
      "2021-02-22 00:19:10,949 P25707 INFO Epoch: 91, loss: 7.03192\n",
      "2021-02-22 00:19:11,201 P25707 INFO Epoch: 92, loss: 6.87431\n",
      "2021-02-22 00:19:11,443 P25707 INFO Epoch: 93, loss: 6.85281\n",
      "2021-02-22 00:19:11,713 P25707 INFO Epoch: 94, loss: 6.88658\n",
      "2021-02-22 00:19:11,966 P25707 INFO Epoch: 95, loss: 6.86356\n",
      "2021-02-22 00:19:12,219 P25707 INFO Epoch: 96, loss: 6.77943\n",
      "2021-02-22 00:19:12,465 P25707 INFO Epoch: 97, loss: 6.81044\n",
      "2021-02-22 00:19:12,704 P25707 INFO Epoch: 98, loss: 6.87955\n",
      "2021-02-22 00:19:12,944 P25707 INFO Epoch: 99, loss: 6.74561\n",
      "2021-02-22 00:19:13,201 P25707 INFO Epoch: 100, loss: 6.81368\n",
      "2021-02-22 00:19:13,437 P25707 INFO Epoch: 101, loss: 6.73793\n",
      "2021-02-22 00:19:13,666 P25707 INFO Epoch: 102, loss: 6.81516\n",
      "2021-02-22 00:19:13,917 P25707 INFO Epoch: 103, loss: 6.80085\n",
      "2021-02-22 00:19:14,163 P25707 INFO Epoch: 104, loss: 6.68373\n",
      "2021-02-22 00:19:14,411 P25707 INFO Epoch: 105, loss: 6.76691\n",
      "2021-02-22 00:19:14,669 P25707 INFO Epoch: 106, loss: 6.78026\n",
      "2021-02-22 00:19:14,914 P25707 INFO Epoch: 107, loss: 6.80197\n",
      "2021-02-22 00:19:15,165 P25707 INFO Epoch: 108, loss: 6.72935\n",
      "2021-02-22 00:19:15,431 P25707 INFO Epoch: 109, loss: 6.75886\n",
      "2021-02-22 00:19:15,674 P25707 INFO Epoch: 110, loss: 6.77811\n",
      "2021-02-22 00:19:15,919 P25707 INFO Epoch: 111, loss: 6.72723\n",
      "2021-02-22 00:19:16,174 P25707 INFO Epoch: 112, loss: 6.76280\n",
      "2021-02-22 00:19:16,436 P25707 INFO Epoch: 113, loss: 6.69489\n",
      "2021-02-22 00:19:16,699 P25707 INFO Epoch: 114, loss: 6.73575\n",
      "2021-02-22 00:19:16,963 P25707 INFO Epoch: 115, loss: 6.69265\n",
      "2021-02-22 00:19:17,245 P25707 INFO Epoch: 116, loss: 6.66706\n",
      "2021-02-22 00:19:17,514 P25707 INFO Epoch: 117, loss: 6.69155\n",
      "2021-02-22 00:19:17,797 P25707 INFO Epoch: 118, loss: 6.66077\n",
      "2021-02-22 00:19:18,046 P25707 INFO Epoch: 119, loss: 6.64958\n",
      "2021-02-22 00:19:18,348 P25707 INFO Epoch: 120, loss: 6.66531\n",
      "2021-02-22 00:19:18,595 P25707 INFO Epoch: 121, loss: 6.67456\n",
      "2021-02-22 00:19:18,881 P25707 INFO Epoch: 122, loss: 6.68584\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-78283119b9e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mtest_iterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mtest_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         )\n\u001b[1;32m      8\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/TS-anomaly/networks/wrappers.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_iterator, test_iterator, test_labels, percent, nb_steps_per_verbose, save_memory, monitor, patience, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;31m# batch: b x d x dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/TS-anomaly/networks/mlstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_window)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mraw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"MEAN\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"TIME\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mtime_inter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFM_interaction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "encoder.fit(\n",
    "            train_iterator,\n",
    "            test_iterator=test_iterator.loader,\n",
    "            test_labels=None,\n",
    "            **params\n",
    "        )\n",
    "encoder.save_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reconstruction_loader(loader, encoder):\n",
    "train_iterator_non_shuffle = WindowIterator(window_dict[\"train_windows\"],\n",
    "                                            batch_size=params[\"batch_size\"], shuffle=False)\n",
    "loader = train_iterator_non_shuffle.loader\n",
    "\n",
    "loader = test_iterator.loader\n",
    "encoder = encoder.eval()\n",
    "with torch.no_grad():\n",
    "    recst_list = []\n",
    "    real_list = []\n",
    "    loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(encoder.device)\n",
    "        return_dict = encoder(batch)\n",
    "        # diff = return_dict[\"diff\"].max(dim=-1)[0] # chose the most anomaous ts\n",
    "        recst = return_dict[\"recst\"]  # chose the most anomaous ts\n",
    "        recst_list.append(recst)\n",
    "        real_list.append(return_dict[\"y\"])\n",
    "        loss += return_dict[\"loss\"].item()\n",
    "print(loss / len(loader))\n",
    "recst_list = torch.cat(recst_list).squeeze()\n",
    "real_list = torch.cat(real_list).squeeze()\n",
    "diff_list = (recst_list - real_list).sigmoid()\n",
    "recst_list = recst_list.cpu().numpy()\n",
    "real_list = real_list.cpu().numpy()\n",
    "\n",
    "score_dict = encoder.score(test_iterator.loader, window_dict[\"test_labels\"])\n",
    "best_f1, best_theta, best_adjust, best_raw = iter_thresholds(\n",
    "            score_dict[\"score\"], score_dict[\"anomaly_label\"]\n",
    "        )\n",
    "# plt.plot(best_raw, \"r\")\n",
    "# plt.plot(best_adjust + 0.1, \"b\")\n",
    "# plt.plot(score_dict[\"anomaly_label\"]+0.3, \"g\")\n",
    "# plt.plot(score_dict[\"score\"], \"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recst_list.shape)\n",
    "print(real_list.shape)\n",
    "print(diff_list.shape)\n",
    "print(window_dict[\"test_labels\"].shape)\n",
    "print(real_list.min(), real_list.max(), recst_list.min(), recst_list.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dims = [9,10,13,14]\n",
    "fig, ax = plt.subplots(nrows=len(dims)+1, sharey=False, figsize=(12,12))\n",
    "i=0\n",
    "# ax[0].plot(recst, 'r', label='reconstruction')\n",
    "# ax[0].legend(loc='best')\n",
    "for i in range(len(dims)):\n",
    "    j = dims[i] \n",
    "    ax[i].plot(recst_list[0:, j].reshape(-1), 'r', label='recst')\n",
    "    ax[i].plot(real_list[0:, j].reshape(-1), 'b', label='real')\n",
    "#     ax[i].plot(diff_list[0:, j].reshape(-1), 'g', label='KPI Error Score')\n",
    "#     ax[i].plot(window_dict[\"test_labels\"][0:, -1].reshape(-1)*0.2+0.4, 'brown', label='Label')\n",
    "    ax[i].legend(loc='best')\n",
    "#     ax[i].set_ylim([0.8,1])\n",
    "# ax[-1].plot(diff_list[:, [5, 6, 10, 14, 18, 29]].max(axis=-1)[0])\n",
    "ax[-1].plot((score_dict[\"score\"]-0.5) * 10, label='Entity error score')\n",
    "ax[-1].plot([(best_theta-0.5)*10] * len(score_dict[\"score\"]), label='Threshold')\n",
    "ax[-1].plot(window_dict[\"test_labels\"][0:, -1].reshape(-1)*0.3, 'brown', label='Label')\n",
    "# ax[-1].set_ylim([0.8,1])\n",
    "# ax[1].legend(loc='best')\n",
    "\n",
    "plt.legend()\n",
    "plt.suptitle(\"Result of the autoencoder\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names=[\"time\", \"expid\", \"dataset\", \"auc\", \"f1\", \"f1a\"]\n",
    "df = pd.read_csv(\"./experiment_results.csv\", sep=\"\\t\", names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9235357142857143"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"f1a\"].map(lambda x: float(x.split(\"-\")[1])).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
